Prequirements:
1.create s3 bucket for backup
2.create kops user in IAM and give admin permission
3.lanuch ec2 instance
4.aws configure secrets keys 
5.ssh-keygen public and private keys to communicate  path  ls ./ssh

####
// installl kubectl 
step 1: install kubectl

curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl.sha256"
echo "$(cat kubectl.sha256)  kubectl" | sha256sum --check
sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl
chmod +x kubectl
mkdir -p ~/.local/bin
mv ./kubectl ~/.local/bin/kubectl
# and then append (or prepend) ~/.local/bin to $PATH
sudo kubectl version --client

#####
// To install Kops :

curl -Lo kops https://github.com/kubernetes/kops/releases/download/$(curl -s https://api.github.com/repos/kubernetes/kops/releases/latest | grep tag_name | cut -d '"' -f 4)/kops-linux-amd64
chmod +x kops
sudo mv kops /usr/local/bin/kops


1. To create a cluser
kops create cluster \
  --name=k8s.deepakdr.site \
  --state=s3://backup.aws123 \
  --zones=us-east-1a,us-east-1b \
  --node-count=2 \
  --node-size=t2.micro \
  --control-plane-size=t3.medium \
  --dns-zone=deepakdr.site \
  --node-volume-size=12 \
  --control-plane-volume-size=12 \
  --ssh-public-key=~/.ssh/id_rsa.pub

or

kops create cluster \
  --name=k8s.deepakdr.site \
  --state=s3://backup.aws123 \
  --zones=us-east-1a,us-east-1b \
  --node-count=2 \
  --node-size=t2.micro \
  --control-plane-size=t3.medium \
  --dns-zone=deepakdr.site \
  --node-volume-size=12 \
  --control-plane-volume-size=12 \
  --ssh-public-key=~/.ssh/id_rsa.pub \
  --topology=public \
  --api-loadbalancer-type=Public



2. To start the cluster update
kops update cluster \
  --name=k8s.deepakdr.site \
  --state=s3://backup.aws123 \
  --yes \
  --admin


3. To validate the cluster is ready or not
kops validate cluster \
  --name=k8s.deepakdr.site \
  --state=s3://backup.aws123


4. Rollback update Terminates and replaces instances (control plane + nodes) one by one
kops rolling-update cluster \
  --name=deepakram.k8s.local \
  --state=s3://backup.aws123 \
  --yes


5. To delete the cluster 
kops delete cluster \
  --name=k8s.deepakdr.site \
  --state=s3://backup.aws123 \
  --yes

Optional UGI K8S

1. If you want a GUI (like Rancher) Install the Kubernetes Dashboard:
   kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.7.0/aio/deploy/recommended.yaml

2. Use LoadBalancer (direct access)  Instead of proxy, expose Dashboard with a LoadBalancer: Change type: ClusterIP ‚ûù type: LoadBalancer.
   kubectl -n kubernetes-dashboard edit service kubernetes-dashboard

3. Check LoadBalancer Status
   kubectl -n kubernetes-dashboard get svc 
  or
   kubectl -n kubernetes-dashboard get svc kubernetes-dashboard

4. Then create a service account and access token:
   kubectl create serviceaccount admin-user -n kubernetes-dashboard
   
5. Bind it to cluster-admin 
    kubectl create clusterrolebinding admin-user-binding \
    --clusterrole=cluster-admin \
    --serviceaccount=kubernetes-dashboard:admin-user

6. Get the login token
   kubectl -n kubernetes-dashboard create token admin-user

7. Access Dashboard Open the ELB DNS you got after changing the service to LoadBalancer:
   kubectl -n kubernetes-dashboard get svc kubernetes-dashboard


url verify
https://a479c8717c2494c6fbnbknlbfknzonlkzsd49e85e02-1489136487.us-east-1.elb.amazonaws.com:443
